name: Deploy Pipeline to GCP

on:
  push:
    branches: [main]
    paths:
      - 'workers/**'
      - 'deface-with-selective-face-blurring/**'
      - 'docker-compose.yml'
      - '.github/workflows/deploy-to-gcp.yml'
  workflow_dispatch:  # Allow manual triggering

env:
  PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  GCR_REGISTRY: gcr.io
  VM_INSTANCE: ${{ secrets.GCP_VM_INSTANCE }}
  VM_ZONE: ${{ secrets.GCP_VM_ZONE }}
  
jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Free up disk space
      run: |
        echo "🧹 Freeing up disk space..."
        
        # Show initial disk usage
        echo "Initial disk usage:"
        df -h
        
        # Remove unnecessary software packages
        sudo apt-get remove -y '^aspnetcore-.*' '^dotnet-.*' '^llvm-.*' '^postgresql-.*' '^mysql-.*' '^mongodb-.*'
        sudo apt-get autoremove -y
        sudo apt-get autoclean
        
        # Remove large directories
        sudo rm -rf /usr/share/dotnet
        sudo rm -rf /usr/local/lib/android
        sudo rm -rf /opt/ghc
        sudo rm -rf /opt/hostedtoolcache/CodeQL
        sudo rm -rf /usr/local/share/boost
        sudo rm -rf /usr/local/graalvm/
        sudo rm -rf /usr/local/share/chromium
        sudo rm -rf /usr/local/share/chromedriver-linux64
        sudo rm -rf /usr/local/share/gecko_driver
        sudo rm -rf /usr/local/share/phantomjs*
        sudo rm -rf /usr/local/share/vcpkg
        sudo rm -rf /usr/local/lib/node_modules
        sudo rm -rf /opt/az
        sudo rm -rf /opt/mssql-tools
        
        # Clear various caches
        sudo rm -rf /var/cache/apt/archives
        sudo rm -rf /tmp/*
        sudo rm -rf /var/tmp/*
        
        # Docker cleanup
        docker system prune -af --volumes || true
        
        # Show disk usage after cleanup
        echo "Disk usage after cleanup:"
        df -h
        
        # Ensure we have at least 10GB free
        available_space=$(df / | awk 'NR==2 {print $4}')
        echo "Available space: ${available_space}KB"
        if [ "$available_space" -lt 10485760 ]; then
          echo "❌ Still not enough space (need ~10GB), trying more aggressive cleanup..."
          
          # More aggressive cleanup
          sudo rm -rf /usr/local/lib/python*
          sudo rm -rf /opt/pipx
          sudo rm -rf /usr/share/doc
          sudo rm -rf /usr/share/man
          sudo rm -rf /var/log/*
          
          # Final check
          df -h
        fi
        
    - name: Authenticate to Google Cloud
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ secrets.GCP_SA_KEY }}

    - name: Set up Cloud SDK
      uses: google-github-actions/setup-gcloud@v2
      with:
        project_id: ${{ secrets.GCP_PROJECT_ID }}
        
    - name: Verify authentication and setup
      run: |
        echo "🔍 Verifying GCP authentication..."
        
        # Check if gcloud is authenticated
        gcloud auth list
        
        # Verify project ID
        echo "Current project: $(gcloud config get-value project)"
        
        # Test GCP access
        gcloud projects describe ${{ secrets.GCP_PROJECT_ID }} || {
          echo "❌ Cannot access project ${{ secrets.GCP_PROJECT_ID }}"
          echo "Please verify GCP_PROJECT_ID secret"
          exit 1
        }
        
        # Enable Container Registry API if not already enabled
        gcloud services enable containerregistry.googleapis.com
        
        # Verify Container Registry access
        gcloud auth configure-docker --quiet
        
        # Test authentication with a simple command (handle empty repository gracefully)
        echo "Testing Container Registry access..."
        gcloud container images list --repository=gcr.io/${{ secrets.GCP_PROJECT_ID }} || {
          echo "⚠️ Container Registry repository is empty or doesn't exist yet (this is normal for new projects)"
          echo "We'll create it when we push the first image"
          
          # Test if we can at least access the project's container registry
          if gcloud services list --enabled --filter="name:containerregistry.googleapis.com" --format="value(name)" | grep -q containerregistry; then
            echo "✅ Container Registry API is enabled"
          else
            echo "❌ Container Registry API not enabled"
            exit 1
          fi
        }
        
        # Verify service account permissions
        echo "🔐 Checking service account permissions..."
        SA_EMAIL=$(gcloud config get-value account)
        echo "Service account: $SA_EMAIL"
        
        # Check if SA has storage admin role (needed for Container Registry)
        if gcloud projects get-iam-policy ${{ secrets.GCP_PROJECT_ID }} --flatten="bindings[].members" --filter="bindings.members:$SA_EMAIL AND bindings.role:roles/storage.admin" --format="value(bindings.role)" | grep -q "roles/storage.admin"; then
          echo "✅ Service account has Storage Admin role"
        else
          echo "⚠️ Service account missing Storage Admin role - adding it now..."
          gcloud projects add-iam-policy-binding ${{ secrets.GCP_PROJECT_ID }} \
            --member="serviceAccount:$SA_EMAIL" \
            --role="roles/storage.admin" || echo "❌ Failed to add Storage Admin role"
        fi
        
        echo "✅ Authentication successful"
        
    - name: Configure Docker for GCR
      run: gcloud auth configure-docker
      
    - name: Build Workers Docker image (smaller first)
      run: |
        echo "🔨 Building Workers image..."
        docker build \
          --no-cache \
          -t $GCR_REGISTRY/$PROJECT_ID/pipeline-workers:$GITHUB_SHA \
          -t $GCR_REGISTRY/$PROJECT_ID/pipeline-workers:latest \
          ./workers
        
        # Show disk usage after workers build
        echo "Disk usage after workers build:"
        df -h
        
    - name: Push Workers image early
      run: |
        echo "📤 Pushing Workers image..."
        docker push $GCR_REGISTRY/$PROJECT_ID/pipeline-workers:$GITHUB_SHA
        docker push $GCR_REGISTRY/$PROJECT_ID/pipeline-workers:latest
        
        # Remove workers image locally to free space
        docker rmi $GCR_REGISTRY/$PROJECT_ID/pipeline-workers:$GITHUB_SHA || true
        docker rmi $GCR_REGISTRY/$PROJECT_ID/pipeline-workers:latest || true
        docker system prune -f
        
        echo "Disk usage after workers push and cleanup:"
        df -h
        
    - name: Build Face Processor Docker image (with aggressive space management)
      run: |
        echo "🔨 Building Face Processor image..."
        
        # Clear any remaining Docker cache
        docker system prune -af --volumes
        
        # Build with limited cache and immediate cleanup
        docker build \
          --no-cache \
          --force-rm \
          -t $GCR_REGISTRY/$PROJECT_ID/pipeline-face-processor:$GITHUB_SHA \
          ./deface-with-selective-face-blurring
        
        # Tag latest
        docker tag $GCR_REGISTRY/$PROJECT_ID/pipeline-face-processor:$GITHUB_SHA \
                   $GCR_REGISTRY/$PROJECT_ID/pipeline-face-processor:latest
        
    - name: Push Face Processor image immediately
      run: |
        echo "📤 Pushing Face Processor image..."
        docker push $GCR_REGISTRY/$PROJECT_ID/pipeline-face-processor:$GITHUB_SHA
        docker push $GCR_REGISTRY/$PROJECT_ID/pipeline-face-processor:latest
        
        # Clean up immediately
        docker rmi $GCR_REGISTRY/$PROJECT_ID/pipeline-face-processor:$GITHUB_SHA || true
        docker rmi $GCR_REGISTRY/$PROJECT_ID/pipeline-face-processor:latest || true
        docker system prune -af --volumes
        
        echo "Final disk usage:"
        df -h

    - name: Create deployment docker-compose
      run: |
        cat > docker-compose.production.yml << 'EOF'
        version: '3.8'
        
        services:
          workers:
            image: ${{ env.GCR_REGISTRY }}/${{ env.PROJECT_ID }}/pipeline-workers:${{ github.sha }}
            container_name: pipeline-workers
            ports:
              - "3000:3000"
            volumes:
              - shared_storage:/app/shared
              - ./logs:/app/logs
            depends_on:
              - face-processor
            restart: unless-stopped
            environment:
              - FACE_PROCESSOR_URL=http://face-processor:5000
            env_file:
              - .env
            healthcheck:
              test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
              interval: 30s
              timeout: 10s
              retries: 3
              start_period: 40s

          face-processor:
            image: ${{ env.GCR_REGISTRY }}/${{ env.PROJECT_ID }}/pipeline-face-processor:${{ github.sha }}
            container_name: pipeline-face-processor
            ports:
              - "5000:5000"
            volumes:
              - shared_storage:/app/shared
            restart: unless-stopped
            environment:
              - PYTHONUNBUFFERED=1
              - CUDA_VISIBLE_DEVICES=0
            deploy:
              resources:
                reservations:
                  devices:
                    - driver: nvidia
                      count: 1
                      capabilities: [gpu]
            healthcheck:
              test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
              interval: 30s
              timeout: 10s
              retries: 3
              start_period: 60s

        volumes:
          shared_storage:
            driver: local
        EOF
        
    - name: Create deployment script
      run: |
        cat > deploy.sh << 'EOF'
        #!/bin/bash
        set -e
        
        echo "🚀 Starting pipeline deployment..."
        
        # Configure Docker for GCR
        gcloud auth configure-docker --quiet
        
        # Stop existing services
        echo "⏹️ Stopping existing services..."
        docker-compose -f docker-compose.production.yml down --remove-orphans || true
        
        # Pull latest images
        echo "📥 Pulling latest images..."
        docker pull ${{ env.GCR_REGISTRY }}/${{ env.PROJECT_ID }}/pipeline-workers:${{ github.sha }}
        docker pull ${{ env.GCR_REGISTRY }}/${{ env.PROJECT_ID }}/pipeline-face-processor:${{ github.sha }}
        
        # Start services
        echo "🆙 Starting services..."
        docker-compose -f docker-compose.production.yml up -d
        
        # Wait for services to start and check logs
        echo "⏳ Waiting for services to start..."
        sleep 10
        
        # Show container status
        echo "📊 Container status:"
        docker-compose -f docker-compose.production.yml ps
        
        # Show container logs for debugging
        echo "📝 Container logs:"
        echo "--- Workers logs ---"
        docker-compose -f docker-compose.production.yml logs --tail=20 workers
        echo "--- Face processor logs ---"
        docker-compose -f docker-compose.production.yml logs --tail=20 face-processor
        
        # More generous wait time for startup
        echo "⏳ Waiting additional time for services to be ready..."
        sleep 45
        
        # Health checks with detailed debugging
        echo "🔍 Checking service health..."
        
        # Check workers service with detailed output
        echo "Testing workers service at http://localhost:3000..."
        
        # Try alternative health checks for workers
        WORKERS_HEALTHY=false
        
        # First check if container is running properly
        if docker-compose -f docker-compose.production.yml ps workers | grep -q "Up"; then
            echo "✅ Workers container: Running"
            
            # Check if port 3000 is accessible (even without auth)
            if curl -s --max-time 5 --connect-timeout 3 http://localhost:3000/ 2>/dev/null | grep -q "Unauthorized\|401\|200"; then
                echo "✅ Workers service: HTTP server responding on port 3000"
                WORKERS_HEALTHY=true
            elif curl -s --max-time 5 --connect-timeout 3 http://localhost:3000/health 2>/dev/null | grep -q "Unauthorized\|401"; then
                echo "✅ Workers service: Health endpoint responding (auth required - this is normal)"
                WORKERS_HEALTHY=true
            else
                echo "⚠️ Workers service: HTTP server may still be starting up"
                # Check if the process is running inside container
                if docker-compose -f docker-compose.production.yml exec -T workers ps aux | grep -q "node\|ts-node"; then
                    echo "✅ Workers service: Node.js process is running"
                    WORKERS_HEALTHY=true
                else
                    echo "❌ Workers service: Node.js process not found"
                fi
            fi
        else
            echo "❌ Workers container: Not running properly"
            echo "Recent workers logs:"
            docker-compose -f docker-compose.production.yml logs --tail=30 workers
        fi
        
        # Check face processor service
        FACE_PROCESSOR_HEALTHY=false
        
        echo "Testing face processor service at http://localhost:5000..."
        
        # First check if container is running properly
        if docker-compose -f docker-compose.production.yml ps face-processor | grep -q "Up"; then
            echo "✅ Face processor container: Running"
            
            # Check the health endpoint (it's not protected by auth)
            if curl -s --max-time 10 --connect-timeout 5 http://localhost:5000/health 2>/dev/null | grep -q '"status".*"healthy"'; then
                echo "✅ Face processor service: Health endpoint responding correctly"
                FACE_PROCESSOR_HEALTHY=true
            elif curl -s --max-time 5 --connect-timeout 3 http://localhost:5000/ 2>/dev/null; then
                echo "✅ Face processor service: HTTP server responding on port 5000"
                FACE_PROCESSOR_HEALTHY=true
            else
                echo "⚠️ Face processor service: HTTP server may still be starting up"
                # Check if Python process is running inside container
                if docker-compose -f docker-compose.production.yml exec -T face-processor ps aux | grep -q "python.*api_service"; then
                    echo "✅ Face processor service: Python Flask process is running"
                    FACE_PROCESSOR_HEALTHY=true
                else
                    echo "❌ Face processor service: Python Flask process not found"
                fi
            fi
        else
            echo "❌ Face processor container: Not running properly"
            echo "Recent face processor logs:"
            docker-compose -f docker-compose.production.yml logs --tail=30 face-processor
        fi
        
        # Final assessment
        if [ "$WORKERS_HEALTHY" = true ] && [ "$FACE_PROCESSOR_HEALTHY" = true ]; then
            echo "✅ Both services are running successfully"
        else
            echo "⚠️ Some services may not be fully ready, but containers are running"
            echo "📊 Final container status:"
            docker-compose -f docker-compose.production.yml ps
            echo "💡 Services may still be starting up. Check logs with:"
            echo "   docker-compose -f docker-compose.production.yml logs -f"
            # Don't fail - let the services continue starting
        fi
        
        echo "🎉 Deployment successful!"
        
        # Show running containers
        echo "📊 Running containers:"
        docker-compose -f docker-compose.production.yml ps
        EOF
        
        chmod +x deploy.sh
        
    - name: Copy files to VM
      run: |
        # Create SSH key files
        echo "${{ secrets.GCP_VM_SSH_KEY }}" > /tmp/ssh_key
        chmod 600 /tmp/ssh_key
        
        # Generate public key from private key
        ssh-keygen -y -f /tmp/ssh_key > /tmp/ssh_key.pub
        chmod 644 /tmp/ssh_key.pub
        
        # Create deployment directory on VM and verify Docker
        gcloud compute ssh $VM_INSTANCE \
          --zone=$VM_ZONE \
          --ssh-key-file=/tmp/ssh_key \
          --strict-host-key-checking=no \
          --quiet \
          --command="
            mkdir -p ~/pipeline-deployment
            
            # Check if Docker is installed and running
            if ! command -v docker &> /dev/null; then
              echo '❌ Docker is not installed on the VM'
              exit 1
            fi
            
            # Check if Docker daemon is running
            if ! docker info &> /dev/null; then
              echo '⚠️ Docker daemon is not running, attempting to start...'
              sudo systemctl start docker || {
                echo '❌ Failed to start Docker daemon'
                exit 1
              }
            fi
            
            # Verify Docker Compose is available
            if ! command -v docker-compose &> /dev/null; then
              echo '❌ Docker Compose is not installed on the VM'
              exit 1
            fi
            
            echo '✅ VM environment verified successfully'
          "
        
        # Copy deployment files
        gcloud compute scp docker-compose.production.yml deploy.sh \
          $VM_INSTANCE:~/pipeline-deployment/ \
          --zone=$VM_ZONE \
          --ssh-key-file=/tmp/ssh_key \
          --strict-host-key-checking=no \
          --quiet
          
    - name: Create environment file on VM
      run: |
        # Create .env file with secrets
        cat > .env.production << 'EOF'
        DATABASE_URL=${{ secrets.DATABASE_URL }}
        GOOGLE_CLIENT_ID=${{ secrets.GOOGLE_CLIENT_ID }}
        GOOGLE_CLIENT_SECRET=${{ secrets.GOOGLE_CLIENT_SECRET }}
        ENCRYPTION_KEY=${{ secrets.ENCRYPTION_KEY }}
        ENCRYPTION_SALT=${{ secrets.ENCRYPTION_SALT }}
        NEXTAUTH_URL=${{ secrets.NEXTAUTH_URL }}
        FACE_PROCESSOR_URL=http://face-processor:5000
        JOB_PROCESSING_INTERVAL=43200000
        TOKEN_CLEANUP_INTERVAL=43200000
        HEALTH_CHECK_PASSWORD=${{ secrets.HEALTH_CHECK_PASSWORD }}
        EOF
        
        # Copy environment file
        gcloud compute scp .env.production \
          $VM_INSTANCE:~/pipeline-deployment/.env \
          --zone=$VM_ZONE \
          --ssh-key-file=/tmp/ssh_key \
          --strict-host-key-checking=no \
          --quiet
          
    - name: Deploy to VM
      run: |
        gcloud compute ssh $VM_INSTANCE \
          --zone=$VM_ZONE \
          --ssh-key-file=/tmp/ssh_key \
          --strict-host-key-checking=no \
          --quiet \
          --command="cd ~/pipeline-deployment && ./deploy.sh"
          
    - name: Setup log monitoring (optional)
      run: |
        gcloud compute ssh $VM_INSTANCE \
          --zone=$VM_ZONE \
          --ssh-key-file=/tmp/ssh_key \
          --strict-host-key-checking=no \
          --quiet \
          --command="
            # Create log monitoring script
            cat > ~/monitor-pipeline.sh << 'MONITOR_EOF'
        #!/bin/bash
        echo '📊 Pipeline Status Check'
        echo '======================='
        
        cd ~/pipeline-deployment
        
        echo '🐳 Container Status:'
        docker-compose -f docker-compose.production.yml ps
        
        echo ''
        echo '💾 Disk Usage:'
        df -h | grep -E '^/dev/'
        
        echo ''
        echo '🖥️ Memory Usage:'
        free -h
        
        echo ''
        echo '🔥 Recent Logs (last 10 lines):'
        echo 'Workers:'
        docker-compose -f docker-compose.production.yml logs --tail=10 workers
        echo ''
        echo 'Face Processor:'
        docker-compose -f docker-compose.production.yml logs --tail=10 face-processor
        MONITOR_EOF
            
            chmod +x ~/monitor-pipeline.sh
            echo '✅ Monitoring script created at ~/monitor-pipeline.sh'
          "
          
    - name: Cleanup
      run: |
        rm -f /tmp/ssh_key /tmp/ssh_key.pub .env.production
        
    - name: Deployment Summary
      run: |
        echo "🎬 Pipeline Deployment Complete!"
        echo "================================="
        echo "Workers Image: $GCR_REGISTRY/$PROJECT_ID/pipeline-workers:$GITHUB_SHA"
        echo "Face Processor Image: $GCR_REGISTRY/$PROJECT_ID/pipeline-face-processor:$GITHUB_SHA"
        echo "VM Instance: $VM_INSTANCE"
        echo "Zone: $VM_ZONE"
        echo ""
        echo "📋 Next Steps:"
        echo "1. Check service health: gcloud compute ssh $VM_INSTANCE --zone=$VM_ZONE --command='~/monitor-pipeline.sh'"
        echo "2. View logs: gcloud compute ssh $VM_INSTANCE --zone=$VM_ZONE --command='cd ~/pipeline-deployment && docker-compose -f docker-compose.production.yml logs -f'"
        echo "3. Add jobs to your database to start processing videos"